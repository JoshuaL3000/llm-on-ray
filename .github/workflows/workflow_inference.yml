name: Inference

on:
  workflow_call:

jobs:
  inference:
    name: inference test
    strategy:
      matrix:
        model: [ gpt-j-6B, gpt2, bloom, opt, mpt ]
        include:
          - dtuner_model: /root/.cache/huggingface/hub/mpt-7b-deltatuner-model
            model: mpt
    runs-on: self-hosted
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Build Docker Image
        run: docker build ./ --build-arg CACHEBUST=1 --build-arg http_proxy=${{ vars.HTTP_PROXY_IMAGE_BUILD }} --build-arg https_proxy=${{ vars.HTTPS_PROXY_IMAGE_BUILD }} -f dev/docker/Dockerfile -t inference:latest && yes | docker container prune && yes | docker image prune

      - name: Start Docker Container
        run: |
          cid=$(docker ps -q --filter "name=inference")
          if [[ ! -z "$cid" ]]; then docker stop $cid && docker rm $cid; fi
          docker run -tid -v /mnt/DP_disk1/huggingface/cache/:/root/.cache/huggingface/hub -v .:/root/llm-ray -e http_proxy=${{ vars.HTTP_PROXY_CONTAINER_RUN }} -e https_proxy=${{ vars.HTTPS_PROXY_CONTAINER_RUN }} --name="inference" --hostname="inference-container" inference:latest

      - name: Start Ray Cluster
        run: |
          docker exec "inference" bash -c "./inference/deepspeed/start-ray-cluster.sh"

      - name: Run Inference Test
        run: |
          docker exec "inference" bash -c "KEEP_SERVE_TERMINAL='false' MODEL_TO_SERVE=\"${{ matrix.model }}\" python inference/run_model_serve.py"
          docker exec "inference" bash -c "python inference/run_model_infer.py --num_iter 1 --model_endpoint http://127.0.0.1:8000/${{ matrix.model }}"
          docker exec "inference" bash -c "python inference/run_model_infer.py --num_iter 1 --model_endpoint http://127.0.0.1:8000/${{ matrix.model }} --streaming_response"
      
      - name: Run Inference Test with Deltatuner
        if: ${{ matrix.dtuner_model }}
        run: |
          docker exec "inference" bash -c "KEEP_SERVE_TERMINAL='false' MODEL_TO_SERVE=\"${{ matrix.model }}\" python inference/run_model_serve.py --deltatuner_model ${{ matrix.dtuner_model }}"
          docker exec "inference" bash -c "python inference/run_model_infer.py --num_iter 1 --model_endpoint http://127.0.0.1:8000/${{ matrix.model }}"
          docker exec "inference" bash -c "python inference/run_model_infer.py --num_iter 1 --model_endpoint http://127.0.0.1:8000/${{ matrix.model }} --streaming_response"

      - name: Run Inference Test with DeepSpeed
        run: |
          if [[ ${{ matrix.model }} =~ ^(gpt2|mpt)$ ]]; then
            echo ${{ matrix.model }} is not supported!
          else
            docker exec "inference" bash -c "KEEP_SERVE_TERMINAL='false' MODEL_TO_SERVE=\"${{ matrix.model }}\" python inference/run_model_serve.py --deepspeed"
            docker exec "inference" bash -c "python inference/run_model_infer.py --num_iter 1 --model_endpoint http://127.0.0.1:8000/${{ matrix.model }}"
            docker exec "inference" bash -c "python inference/run_model_infer.py --num_iter 1 --model_endpoint http://127.0.0.1:8000/${{ matrix.model }} --streaming_response"
          fi

      - name: Run Inference Test with DeepSpeed and Deltatuner
        if: ${{ matrix.dtuner_model }}
        run: |
          if [[ ${{ matrix.model }} =~ ^(gpt2|mpt)$ ]]; then
            echo ${{ matrix.model }} is not supported!
          else
            docker exec "inference" bash -c "KEEP_SERVE_TERMINAL='false' MODEL_TO_SERVE=\"${{ matrix.model }}\" python inference/run_model_serve.py --deepspeed --deltatuner_model ${{ matrix.dtuner_model }}"
            docker exec "inference" bash -c "python inference/run_model_infer.py --num_iter 1 --model_endpoint http://127.0.0.1:8000/${{ matrix.model }}"
            docker exec "inference" bash -c "python inference/run_model_infer.py --num_iter 1 --model_endpoint http://127.0.0.1:8000/${{ matrix.model }} --streaming_response"
          fi

      - name: Stop Container
        if: success() || failure()
        run: |
          cid=$(docker ps -q --filter "name=inference")
          if [[ ! -z "$cid" ]]; then docker stop $cid && docker rm $cid; fi

      - name: Test Summary
        run: echo "to be continued"