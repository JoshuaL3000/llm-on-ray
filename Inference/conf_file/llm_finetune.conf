{'run_mode': 'ray', 'seed': 42, 'torch_thread_num': 24, 'logging_config_file': '/home/ykp/project/llm-ray/Finetune/logging.conf', 'logger_name': 'custom', 'accelerator': {'gradient_accumulation_steps': 1}, 'datasets': {'type': 'HuggingfaceDataset', 'name': '/mnt/DP_disk3/ykp/dataset/wikitext', 'load_from_disk': True, 'load_config': {}}, 'tokenizer': {'type': 'HuggingFaceTokenizer', 'name': '/mnt/DP_disk3/ykp/huggingface/gpt2', 'config': {}}, 'model': {'type': 'HuggingFaceModelForCausalLM', 'name': '/mnt/DP_disk3/ykp/huggingface/gpt2', 'config': {}}, 'optimizer': {'type': 'DefaultOptimizer', 'name': 'AdamW', 'config': {'lr': 1e-05}}, 'trainer': {'type': 'DefaultTrainer', 'num_train_epochs': 1, 'max_train_step': 1, 'max_eval_step': 0, 'output': '/mnt/DP_disk3/ykp/model_save/finetuned/my_alpaca', 'dataprocesser': {'type': 'WikitextProcesser', 'preprocessing_num_workers': 4, 'batched': True, 'batch_size': 1000, 'block_size': 1024, 'overwrite_cache': True, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 4, 'shuffle': False}, 'lr_scheduler': {'enable': False, 'max_train_steps': None, 'lr_scheduler_type': 'linear', 'num_warmup_steps': 0}, 'checkpoint': None}, 'ray_config': {'init': {'runtime_env': {'env_vars': {'OMP_NUM_THREADS': '24', 'ACCELERATE_USE_CPU': 'True', 'ACCELERATE_USE_FSDP': 'false', 'FSDP_SHARDING_STRATEGY': '1', 'FSDP_OFFLOAD_PARAMS': 'false', 'FSDP_MIN_NUM_PARAMS': '1000000', 'FSDP_AUTO_WRAP_POLICY': 'SIZE_BASED_WRAP', 'FSDP_BACKWARD_PREFETCH': 'BACKWARD_PRE', 'FSDP_STATE_DICT_TYPE': 'SHARDED_STATE_DICT', 'ACCELERATE_MIXED_PRECISION': 'no', 'CCL_WORKER_COUNT': '2', 'CCL_LOG_LEVEL': 'info', 'WORLD_SIZE': '2', 'FI_PROVIDER': 'tcp', 'FI_TCP_IFACE': 'eth2', 'KMP_AFFINITY': 'granularity=fine,compact'}}, 'address': 'auto', '_node_ip_address': '10.1.0.133'}, 'scaling_config': {'num_workers': 2, 'resources_per_worker': {'CPU': 24}, 'placement_strategy': 'SPREAD'}, 'torch_config': {'backend': 'ccl'}, 'failure_config': {'max_failures': 0}, 'run_config': {'local_dir': '.'}}}