# I am python, not json
{
    # "run_mode" : "ray",
    #"run_mode" : "standalone",
    "run_mode": "initialized",
    "seed": 42,
    "torch_thread_num": 24,
    "logging_config_file": "./logging.conf",
    "logger_name": "custom",
    "accelerator": {
        "gradient_accumulation_steps": 1,
    },
    "datasets": {
        "type": "HuggingfaceDataset",
        "name": "/panfs/users/liangyul/workspace/llm-ray/data/huggingface/wikitext-2-raw",
        "load_from_disk": True,
        "load_config" : {}
    },
    "tokenizer": {
        "type": "HuggingFaceTokenizer",
        "name": "/panfs/users/liangyul/workspace/llm-ray/tokenizer/EleutherAI/gpt-j-6B",
        # "name": "/panfs/users/liangyul/workspace/llm-ray/tokenizer/EleutherAI/pythia-2.8b",
        # "name": "/panfs/users/liangyul/workspace/llm-ray/tokenizer/bigscience/bloom-560m",
        # "name": "/panfs/users/liangyul/workspace/llm-ray/tokenizer/facebook/opt-125m",
        "config": {}
    },
    "model": {
        "type": "HuggingFaceModelForCausalLM",
        "name": "/lfs/lfs12/liangyul/workspace/alpaca/model/EleutherAI/gpt-j-6B",
        # "name": "/lfs/lfs12/liangyul/workspace/alpaca/model/EleutherAI/pythia-2.8b",
        # "name": "/lfs/lfs12/liangyul/workspace/alpaca/model/bigscience/bloom-560m",
        # "name": "/lfs/lfs12/liangyul/workspace/alpaca/model/facebook/opt-125m",
        "config": {}
    },
    "optimizer": {
        "type": "DefaultOptimizer",
        "name": "AdamW",
        "config": {
            "lr" : 1e-5,
        }
    },
    "trainer": {
        "type": "DefaultTrainer",
        "num_train_epochs": 1,
        "max_train_step": 1,
        "max_eval_step": 0,
        "output": "/lfs/lfs12/liangyul/workspace/output",
        "dataprocesser": {
            "type": "WikitextProcesser",
            "preprocessing_num_workers": 4,
            "batched": True,
            "batch_size": 1000,
            "block_size": 1024,
            "overwrite_cache": True,
            "per_device_train_batch_size": 4,
            "per_device_eval_batch_size": 4,
            "shuffle": False
        },
        "lr_scheduler": {
            "enable": False,
            "max_train_steps": None,
            "lr_scheduler_type": "linear",
            "num_warmup_steps": 0,
        },
        "checkpoint": None
    },
    "ray_config": {
        "init": {
            "runtime_env": {
                "env_vars": {
                    "OMP_NUM_THREADS": "24", 
                    "ACCELERATE_USE_CPU": "True", 
                    "ACCELERATE_USE_FSDP": "false",  # FSDP setting
                    "FSDP_SHARDING_STRATEGY": "1", 
                    "FSDP_OFFLOAD_PARAMS": "false", 
                    "FSDP_MIN_NUM_PARAMS": "1000000", 
                    "FSDP_AUTO_WRAP_POLICY": "SIZE_BASED_WRAP", 
                    "FSDP_BACKWARD_PREFETCH": "BACKWARD_PRE", 
                    "FSDP_STATE_DICT_TYPE": "SHARDED_STATE_DICT", 
                    "ACCELERATE_MIXED_PRECISION": "no",
                    "CCL_WORKER_COUNT": "2",        # CCL setting
                    "CCL_LOG_LEVEL": "info",
                    "WORLD_SIZE": "2",    # Enable multi-process
                    "FI_PROVIDER": "tcp",         # Network setting
                    "FI_TCP_IFACE": "eth2",
                    # "LD_LIBRARY_PATH": "/usr/local/ofed/mlnx-5.9-0.5.6.0_372.32.1.3_2.12.9/lib64:/usr/local/ofed/mlnx-5.9-0.5.6.0_372.32.1.3_2.12.9/lib64/libibverbs",
                    "KMP_AFFINITY": "granularity=fine,compact",
                }
            },
            "address": "auto",
            "_node_ip_address": "10.1.0.133",
        },
        "scaling_config": {
            "num_workers": 2,
            "resources_per_worker": {
                "CPU": 24
            },
            "placement_strategy": "SPREAD"
        },
        "torch_config": {
            "backend" : "ccl",
        },
        "failure_config": {
            "max_failures": 0
        },
        "run_config": {
            "local_dir": "/tmp/ray_results"
        }
    }
}

