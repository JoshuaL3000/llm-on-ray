From f5b68a271939438ef264b65149708a748e597abb Mon Sep 17 00:00:00 2001
From: zhangjian <jian2.zhang@intel.com>
Date: Mon, 27 Nov 2023 19:49:12 +0800
Subject: [PATCH] hot fix for megatron deepspeed

---
 megatron/initialize.py |  3 +++
 megatron/training.py   | 14 +++++++++-----
 2 files changed, 12 insertions(+), 5 deletions(-)

diff --git a/megatron/initialize.py b/megatron/initialize.py
index 3aa035a..adbea3d 100644
--- a/megatron/initialize.py
+++ b/megatron/initialize.py
@@ -208,6 +208,9 @@ def _initialize_distributed():
                   'skipping initialization ...', flush=True)
         args.rank = torch.distributed.get_rank()
         args.world_size = torch.distributed.get_world_size()
+        
+        device = args.rank % device_count
+        get_accelerator().set_device(device)
 
     else:
         if args.rank == 0:
diff --git a/megatron/training.py b/megatron/training.py
index c509377..f6cbbe5 100644
--- a/megatron/training.py
+++ b/megatron/training.py
@@ -103,7 +103,8 @@ def pretrain(train_valid_test_dataset_provider,
 
     # Initalize and get arguments, timers, and Tensorboard writer.
     initialize_megatron(extra_args_provider=extra_args_provider,
-                        args_defaults=args_defaults)
+                        args_defaults=args_defaults,
+                        ignore_unknown_args=True)
     # Set pytorch JIT layer fusion options and warmup JIT functions.
     if get_accelerator().device_name() == 'cuda':
         set_jit_fusion_options()
@@ -113,8 +114,8 @@ def pretrain(train_valid_test_dataset_provider,
     # image ... launches.
     global _TRAIN_START_TIME
     start_time_tensor = get_accelerator().DoubleTensor([_TRAIN_START_TIME])
-    torch.distributed.all_reduce(start_time_tensor,
-                                 op=torch.distributed.ReduceOp.MIN)
+    # torch.distributed.all_reduce(start_time_tensor,
+    #                              op=torch.distributed.ReduceOp.MIN)
     _TRAIN_START_TIME = start_time_tensor.item()
     print_rank_0('time to initialize megatron (seconds): {:.3f}'.format(
         time.time() - _TRAIN_START_TIME))
@@ -124,8 +125,11 @@ def pretrain(train_valid_test_dataset_provider,
     timers = get_timers()
 
     if args.deepspeed:
-        args.deepspeed_configuration = json.load(
-            open(args.deepspeed_config, 'r', encoding='utf-8'))
+        if isinstance(args.deepspeed_config, dict) :
+            args.deepspeed_configuration = args.deepspeed_config
+        else:
+            args.deepspeed_configuration = json.load(
+                open(args.deepspeed_config, 'r', encoding='utf-8'))
         if "curriculum_learning" in args.deepspeed_configuration and \
             "enabled" in args.deepspeed_configuration["curriculum_learning"]:
             args.curriculum_learning_legacy = args.deepspeed_configuration[ \
-- 
2.25.1

